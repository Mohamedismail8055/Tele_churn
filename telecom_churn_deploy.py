# -*- coding: utf-8 -*-
"""Telecom_Churn_Model_building (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzRpAc03NFJDSf9dbM1dDNJ--gM7NO4q

#Telecom Customer Churn Prediction
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import SMOTE
import os

"""# Importing the dataset"""

import subprocess
subprocess.run(["pip", "install", "openpyxl"], check=True) # since the dataset file is of .xlsx, not csv.

df = pd.read_excel('churn1.xlsx',engine="openpyxl")
df.head()

# Dropping unnecessary column "Unnamed"
df=df.iloc[:,1:]

# first 5 rows of data
df.head()

# Drop the 'state' column
df = df.drop('state', axis=1)

# information of the dataset
df.info()

""" ### It gives us all columns names, it's data type. Here we can observe that two features daycharge and evening minutes are getting wrong data type. So we will convert it into correct data type."""

df['day.charge']=pd.to_numeric(df['day.charge'],errors='coerce')
df['eve.mins']=pd.to_numeric(df['eve.mins'],errors='coerce')

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

columns_to_encode = ['area.code', 'voice.plan', 'intl.plan', 'churn']

# Create mapping dictionaries
mapping_dict = {}

for col in columns_to_encode:
    df[col] = le.fit_transform(df[col])
    # Create a mapping dictionary for the current column
    mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    mapping_dict[col] = mapping

# Print the mapping dictionaries
mapping_dict

df.isnull().sum()

# median imputation

# we can fill null values by mean but we have outliers so median is best prefered.

df['day.charge']=df['day.charge'].fillna(df['day.charge'].median())
df['eve.mins']=df['eve.mins'].fillna(df['eve.mins'].median())

df.isnull().sum()

df.info()

# Handling imbalance with SMOTE
X = df.drop(columns=['churn'])
y = df['churn']
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Recreate DataFrame
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['churn'] = y_resampled

# Outlier removal using 3-sigma rule, excluding 'voice.plan', 'intl.plan', 'churn'
for col in df_resampled.select_dtypes(include=np.number).columns:
    if col != ('voice.plan', 'intl.plan', 'churn'):  # Exclude 'voice.plan', 'intl.plan', 'churn' column. Even if its not excluded also, no issue. Since boolean columns are not considered.
        mean = df_resampled[col].mean()
        std = df_resampled[col].std()
        df_resampled = df_resampled[(df_resampled[col] > mean - 3 * std) & (df_resampled[col] < mean + 3 * std)]

from sklearn.preprocessing import MinMaxScaler

numerical_features = df_resampled.select_dtypes(include=np.number).columns
scaler = MinMaxScaler()
normalised_data = scaler.fit_transform(df_resampled[numerical_features])
normalised_df = pd.DataFrame(normalised_data, columns=numerical_features, index=df_resampled.index)
normalised_df.head()

X=normalised_df.iloc[:,:-1]
y=normalised_df.iloc[:,-1]

X_rfe = normalised_df.drop(columns=['churn'])
y_rfe = normalised_df['churn']

"""# Collinearity check and feature selection"""

def select_features_by_collinearity(df, threshold=0.85, top_n=10):

    corr_matrix = df.corr()
    correlated_features = set()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]
                correlated_features.add(colname)

    # Calculate feature importances (example using Random Forest)
    from sklearn.ensemble import RandomForestClassifier
    X = df.drop(columns=['churn'])  # 'churn' is target variable
    y = df['churn']
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X, y)
    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)

    # Filter out correlated features and sort by importance
    filtered_importances = feature_importances.drop(correlated_features)
    top_features = filtered_importances.sort_values(ascending=False).head(top_n).index.tolist()

    return top_features

top_ten_features = select_features_by_collinearity(df_resampled, threshold=0.85, top_n=10)
top_ten_features


# Splitting into X and y based on collinearty check feature selection, although i can choose mutual info score or nonlinear pattern, i selected rfe for feature selection
X_colinearity_selected = normalised_df[['day.mins',
 'customer.calls',
 'eve.mins',
 'voice.plan',
 'night.mins',
 'account.length',
 'intl.mins',
 'night.calls',
 'day.calls',
 'eve.calls']]
y = normalised_df['churn']

"""X TRAIN AND Y TRAIN SPLITTING"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_colinearity_selected, y, test_size=0.2, random_state=42)



"""# **MODEL BUILDING**

LOGISTIC REGRESSION
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the logistic regression model
logreg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence
logreg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""TUNED XGBOOST MODEL"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb

# Define fixed parameters for XGBoost
xgb_params = {
    'n_estimators': 300,
    'learning_rate': 0.1,
    'max_depth': 10,
    'subsample': 0.8,
    'use_label_encoder': False,
    'eval_metric': 'mlogloss',
    'random_state': 42
}

# Initialize and train the XGBoost model
xgb_classifier = xgb.XGBClassifier(**xgb_params)
xgb_classifier.fit(X_train, y_train)

# Evaluate on test data
y_pred_xgb = xgb_classifier.predict(X_test)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)

# Print results
print(f"XGBoost Accuracy: {accuracy_xgb:.4f}")
print(classification_report(y_test, y_pred_xgb))
print(confusion_matrix(y_test, y_pred_xgb))


"""TUNED GRADIENT BOOSTING MODEL"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define fixed parameters for Gradient Boosting
gb_params = {
    'n_estimators': 500,  # Number of boosting rounds
    'learning_rate': 0.3,  # Step size
    'max_depth': 9,  # Tree depth
    'min_samples_split': 15,  # Min samples to split a node
    'min_samples_leaf': 4,  # Min samples per leaf
    'subsample': 0.9,  # Fraction of samples per iteration
    'max_features': None,  # Features per split
    'random_state': 42
}

# Initialize and train the Gradient Boosting model
gb_classifier = GradientBoostingClassifier(**gb_params)
gb_classifier.fit(X_train, y_train)

# Evaluate on test data
y_pred_gb = gb_classifier.predict(X_test)
accuracy_gb = accuracy_score(y_test, y_pred_gb)

# Print results
print(f"Gradient Boosting Accuracy: {accuracy_gb:.4f}")
print(classification_report(y_test, y_pred_gb))
print(confusion_matrix(y_test, y_pred_gb))


import subprocess

# Install dependencies from requirements.txt
subprocess.check_call(['pip', 'install', '-r', 'requirements.txt'])

"""## Importing Libraries"""

import joblib

# Assuming your trained models are named logistic_model, xgboost_model, and gb_model
joblib.dump(logreg, "logistic_model.pkl")
joblib.dump(xgb_classifier, "tuned_xgboost_model.pkl")
joblib.dump(gb_classifier, "tuned_gradient_boosting_model.pkl")
joblib.dump(scaler, 'scaler.pkl')

# DEPLOYMENT

import streamlit as st
import joblib
import numpy as np
import pandas as pd

# Load trained models
logistic_model = joblib.load("logistic_model.pkl")
xgboost_model = joblib.load("tuned_xgboost_model.pkl")
gb_model = joblib.load("tuned_gradient_boosting_model.pkl")
scaler = joblib.load("scaler.pkl")  # Load the scaler

# Streamlit UI
st.title("Telecom Churn Prediction App")
st.write("Enter feature values to predict churn.")

# Model selection
model_choice = st.selectbox("Choose a model:", 
                            ["Logistic Regression", "Tuned XGBoost", "Tuned Gradient Boosting (Highest Accuracy)"])

# User input fields (Top 10 Features Only)
    day_mins: st.slider("Day Minutes", min_value=0.0, max_value=300.0, step=0.1, value=150.0),
    customer_calls: st.slider("Customer Calls", min_value=0, max_value=20, step=1, value=5),
    eve_mins: st.slider("Evening Minutes", min_value=0.0, max_value=300.0, step=0.1, value=150.0),
    voice_plan: st.radio("Voice Plan", options=[0, 1], index=0),
    night_mins: st.slider("Night Minutes", min_value=0.0, max_value=300.0, step=0.1, value=150.0),
    account_length: st.slider("Account Length", min_value=1, max_value=243, step=1, value=100),
    intl_mins: st.slider("International Minutes", min_value=0.0, max_value=20.0, step=0.1, value=10.0),
    night_calls: st.slider("Night Calls", min_value=0, max_value=160, step=1, value=75),
    day_calls: st.slider("Day Calls", min_value=0, max_value=160, step=1, value=75),
    eve_calls: st.slider("Evening Calls", min_value=0, max_value=160, step=1, value=75)

input data={
    'day.mins':day_mins,
    'customer.calls':customer_calls,
    'eve.mins':eve_mins,
    'voice.plan':voice_plan,
    'night.mins':night_mins,
    'account.length':account_length,
    'intl.mins':intl_mins,
    'night.calls':night_calls,
    'day.calls': day_calls,
    'eve.calls':eve_calls
}
    
    

# Convert user input to DataFrame
input_data = pd.DataFrame([input_data])


# Standardize input data
input_data_scaled = scaler.transform(input_data)

# Prediction
if st.button("Predict Churn"):
    if model_choice == "Logistic Regression":
        model = logistic_model
    elif model_choice == "Tuned XGBoost":
        model = xgboost_model
    elif model_choice == "Tuned Gradient Boosting (Highest Accuracy)":
        model = gb_model
    else:
        model = rand_forest_model
    
    prediction = model.predict(input_data_scaled)
    st.success(f"The predicted churn is {prediction[0]}")

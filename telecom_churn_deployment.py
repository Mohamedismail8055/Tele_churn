# -*- coding: utf-8 -*-
"""Telecom_Churn_Model_building (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jCsIvpYUguNoKujLp-v3zvNHDBA8hVr

#Telecom Customer Churn Prediction
"""

import subprocess

# Install dependencies from requirements.txt
subprocess.check_call(['pip', 'install', '-r', 'requirements.txt'])

"""## Importing Libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import SMOTE
import os

"""# Importing the dataset"""

df = pd.read_excel('/content/churn1.xlsx')
df.head()

# Dropping unnecessary column "Unnamed"
df=df.iloc[:,1:]

# first 5 rows of data
df.head()

# Drop the 'state' column
df = df.drop('state', axis=1)

# information of the dataset
df.info()

""" ### It gives us all columns names, it's data type. Here we can observe that two features daycharge and evening minutes are getting wrong data type. So we will convert it into correct data type."""

df['day.charge']=pd.to_numeric(df['day.charge'],errors='coerce')
df['eve.mins']=pd.to_numeric(df['eve.mins'],errors='coerce')

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

columns_to_encode = ['area.code', 'voice.plan', 'intl.plan', 'churn']

# Create mapping dictionaries
mapping_dict = {}

for col in columns_to_encode:
    df[col] = le.fit_transform(df[col])
    # Create a mapping dictionary for the current column
    mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    mapping_dict[col] = mapping

# Print the mapping dictionaries
mapping_dict

df.isnull().sum()

# median imputation

# we can fill null values by mean but we have outliers so median is best prefered.

df['day.charge']=df['day.charge'].fillna(df['day.charge'].median())
df['eve.mins']=df['eve.mins'].fillna(df['eve.mins'].median())

df.isnull().sum()

df.info()

# Handling imbalance with SMOTE
X = df.drop(columns=['churn'])
y = df['churn']
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Recreate DataFrame
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['churn'] = y_resampled

# description of the dataset
df_resampled.describe()

# varaince of all numerical columns
df.var(numeric_only=True)

# skewness
df.skew(numeric_only=True)

# checking for duplicates if there is any
df.duplicated().any()

df[df.duplicated(keep=False)]

"""### We can see the mean values of each column grouped by the churn."""

# two way tab
pd.crosstab(df['churn'],df['intl.plan'],margins=True)

# Voice plan
pd.crosstab(df['churn'],df['voice.plan'],margins=True)

# Customer service calls
pd.crosstab(df['churn'],df["customer.calls"],margins=True)

numerical_cols = df_resampled.select_dtypes(include=np.number).columns
num_columns = len(numerical_cols)

plt.figure(figsize=(20, 20))

# Calculate rows and columns to fit all subplots
rows = (num_columns // 4) + (num_columns % 4 > 0)

for i, col in enumerate(numerical_cols):
    plt.subplot(rows, 4, i + 1)  # Adjust grid layout as needed
    sns.boxplot(y=df[col])
    plt.title(col)
    plt.tight_layout()

plt.show()

# Find columns in df_resampled with only 0s and 1s
binary_cols = []
for col in df_resampled.columns:
  if set(df_resampled[col].unique()) == {0, 1}:
    binary_cols.append(col)
print("Columns with only 0 and 1:", binary_cols)

# Outlier removal using 3-sigma rule, excluding 'voice.plan', 'intl.plan', 'churn'
for col in df_resampled.select_dtypes(include=np.number).columns:
    if col != ('voice.plan', 'intl.plan', 'churn'):  # Exclude 'voice.plan', 'intl.plan', 'churn' column. Even if its not excluded also, no issue. Since boolean columns are not considered.
        mean = df_resampled[col].mean()
        std = df_resampled[col].std()
        df_resampled = df_resampled[(df_resampled[col] > mean - 3 * std) & (df_resampled[col] < mean + 3 * std)]

plt.figure(figsize=(20, 15))  # Adjust figure size as needed
num_columns = df_resampled.select_dtypes(include=np.number).shape[1]

# Calculate rows and columns to fit all subplots
rows = (num_columns // 3) + (num_columns % 3 > 0)

for i, col in enumerate(df_resampled.select_dtypes(include=np.number).columns):
    plt.subplot(rows, 3, i + 1)  # Adjust grid layout as needed
    sns.boxplot(y=df_resampled[col])
    plt.title(col)

plt.tight_layout()
plt.show()

df_resampled  # dataframe with outliers removed

"""## Data Visualization
### To show the Percentage of churn using pie chart

* Before resampling
"""

plt.pie(df['churn'].value_counts(), labels = df['churn'].unique(), autopct='%1.1f%%',startangle=90,
         explode=(0.3,0),
         radius=1.2,
         textprops={'fontsize': 10, 'color':'k'},
         wedgeprops={'linewidth': 1, 'edgecolor': 'white'},
         center=(0,0),
         shadow=True,
         colors= sns.color_palette('Set2'))
plt.title("Before resampling")
plt.show()

"""* After resampling"""

plt.pie(df_resampled['churn'].value_counts(), labels = df_resampled['churn'].unique(), autopct='%1.1f%%',startangle=90,
         explode=(0.3,0),
         radius=1.2,
         textprops={'fontsize': 10, 'color':'k'},
         wedgeprops={'linewidth': 1, 'edgecolor': 'white'},
         center=(0,0),
         shadow=True,
         colors= sns.color_palette('Set2'))
plt.title("After resampling")
plt.show()

"""###  To show the Percentage of Charges during different time of the day using Pie Char"""

plt.figure(figsize = (5,5))
 #sns.set(rc={'figure.figsize':(3,4)})
 charge = np.array([df_resampled['intl.charge'].sum(), df_resampled['day.charge'].sum(), df_resampled['eve.charge'].sum(), df_resampled['night.charge'].sum()])
 plt.pie(charge,
         labels = ['International charge', 'Day charge', 'Evening charge','Night charge'],
         autopct= "%1.1f%%",
         explode =(0.2,0,0,0),
         radius=1.2,
         textprops={'fontsize': 10, 'color': '#5CD9BB'},
         shadow=True,
         colors =sns.color_palette("viridis"))
 plt.title("Total charge")
 plt.show()

sns.pairplot(df_resampled,hue='churn')

"""### In pairplot we can visualise the highly correlared columns like intl.mins &intl.charge and day.mins & day charge"""

plt.figure(figsize = (5,5))
pal = sns.color_palette("pastel")
sns.catplot(x="area.code", y="day.charge", data=df_resampled, kind='bar',hue='churn')
plt.xticks(rotation = 0, fontsize = 10)
plt.show()

#Correlation
df_resampled.corr(numeric_only=True)

plt.figure(figsize=(18,15))
 corr = df_resampled.corr(numeric_only=True)
 sns.heatmap(corr, annot=True,fmt ="0.2f")
 plt.show()

"""### Light color shows that they are strongly correlated"""

# Histogram for each variable
df_resampled.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)

"""# Insights
 Almost every features are weakly correlated.
 Almost all features are normally distributed except voice message, init calls, customer calls.
 International minutes and International charges, night minutes and night charges, Evening
 minutes and Evening charges are strongly positively correlated. As minutes of the talking
 time increases, charges also increases.

# t-sne visualisation
"""

X_df_resampled = df_resampled.drop('churn', axis=1)
y_df_resampled = df_resampled['churn']

from sklearn.preprocessing import StandardScaler
scaler3 = StandardScaler()
X_scaled = scaler3.fit_transform(X_df_resampled)

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_embedded = tsne.fit_transform(X_scaled)
tsne_df = pd.DataFrame(X_embedded, columns=['TSNE1', 'TSNE2'])
tsne_df['churn'] = y_df_resampled.values

plt.figure(figsize=(10, 6))
sns.scatterplot(x='TSNE1', y='TSNE2', hue='churn', palette='coolwarm', data=tsne_df)
plt.title("t-SNE Visualization of Customer Data")
plt.show()

# Collinearity check using correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(df_resampled.corr(), cmap='coolwarm', annot=True)
plt.title("Feature Correlation Matrix")
plt.show()

"""#Mutual Information Score"""

# Mutual Information score, alternative for PPS matrix using correlation
def calculate_mutual_information(df, target):
    from sklearn.feature_selection import mutual_info_classif
    X = df.drop(columns=[target])
    y = df[target]
    mi = mutual_info_classif(X, y, discrete_features='auto')
    return pd.Series(mi, index=X.columns)

mutual_info_scores = calculate_mutual_information(df_resampled, 'churn')
print("Top features based on mutual information:")
print(mutual_info_scores.sort_values(ascending=False).head(10))

# The mutual information scores are printed in descending order, showing the top 10 features based on mutual information.

"""**The above code prints the top 10 features based on their mutual information scores with the target variable 'churn'. These features are considered the most informative for predicting whether a customer will churn.**

# Non-Linear Pattern for finding top predicting features
"""

# Detect non-linear patterns using Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_resampled, y_resampled)
feature_importances = pd.Series(rf.feature_importances_, index=X.columns)
print("Top features with non-linear patterns:")
print(feature_importances.sort_values(ascending=False).head(10))

"""# Collinearity check and feature selection"""

def select_features_by_collinearity(df, threshold=0.85, top_n=10):

    corr_matrix = df.corr()
    correlated_features = set()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]
                correlated_features.add(colname)

    # Calculate feature importances (example using Random Forest)
    from sklearn.ensemble import RandomForestClassifier
    X = df.drop(columns=['churn'])  # 'churn' is target variable
    y = df['churn']
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X, y)
    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)

    # Filter out correlated features and sort by importance
    filtered_importances = feature_importances.drop(correlated_features)
    top_features = filtered_importances.sort_values(ascending=False).head(top_n).index.tolist()

    return top_features

top_ten_features = select_features_by_collinearity(df_resampled, threshold=0.85, top_n=10)
top_ten_features

numerical_features = ['day.mins', 'customer.calls', 'eve.mins', 'voice.plan', 'night.mins', 'account.length', 'intl.mins', 'night.calls', 'day.calls', 'eve.calls']
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
normalised_data = scaler.fit_transform(df_resampled[numerical_features])
normalised_df = pd.DataFrame(normalised_data, columns=numerical_features, index=df_resampled.index)
normalised_df.head()

"""# RECURSIVE FEATURE ELIMINATION"""

X = df_resampled.drop('churn', axis=1)
y = df_resampled['churn']


from sklearn.preprocessing import StandardScaler
scaler2 = StandardScaler()
X_scaled = scaler2.fit_transform(X)

# Recursive Feature Elimination
estimator = LogisticRegression() # Or any other suitable estimator
selector = RFE(estimator, n_features_to_select=10, step=1) # Adjust n_features_to_select as needed
selector = selector.fit(X_scaled, y)

# Print the selected features
X.columns[selector.support_]

"""These features have been identified by RFE as the most relevant for your predictive model, potentially improving its accuracy and performance.

# Top features based on
1. **mutual information:**
intl.charge   :   0.429700,
intl.mins     :   0.426242,
night.charge  :    0.221482,
day.charge    :    0.144500,
day.mins      :    0.129421,
eve.charge    :    0.100247,
eve.mins      :    0.066443,
night.mins    :    0.054718,
voice.plan    :    0.050531,
voice.messages:    0.032908


2. **non-linear patterns:**
 day.charge     :   0.141592,
 day.mins       :   0.125520,
 customer.calls :   0.095396,
 eve.mins       :   0.061251,
 eve.charge     :   0.059580,
 voice.messages :   0.053899,
 voice.plan     :   0.052052,
 night.charge   :   0.045311,
 intl.charge    :   0.045038,
 night.mins     :   0.043922

3. **RFE:**
 area.code, voice.plan, voice.messages, intl.mins, intl.calls,
       day.mins, day.charge, eve.charge, night.charge,
       customer.calls

4. **Feature selection based on collinearity check.**
  ['day.mins',
 'customer.calls',
 'eve.mins',
 'voice.plan',
 'night.mins',
 'account.length',
 'intl.mins',
 'night.calls',
 'day.calls',
 'eve.calls']
"""

# Splitting into X and y based on collinearty check feature selection, although i can choose mutual info score or nonlinear pattern, i selected rfe for feature selection
X_colinearity_selected = normalised_df[['day.mins',
 'customer.calls',
 'eve.mins',
 'voice.plan',
 'night.mins',
 'account.length',
 'intl.mins',
 'night.calls',
 'day.calls',
 'eve.calls']]
y = df_resampled['churn']

X_colinearity_selected.head()

y.head()

X_colinearity_selected.shape,y.shape

"""X TRAIN AND Y TRAIN SPLITTING"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_colinearity_selected, y, test_size=0.2, random_state=42)



"""# **MODEL BUILDING**

LOGISTIC REGRESSION
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the logistic regression model
logreg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence
logreg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""ROC CURVE AND ROC SCORE"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Get predicted probabilities for the positive class
y_probs = logreg.predict_proba(X_test)[:, 1]

# Compute the ROC curve
fpr, tpr, _ = roc_curve(y_test, y_probs)

# Compute the AUC score
roc_auc = roc_auc_score(y_test, y_probs)

# Print AUC score
print(f"ROC AUC Score: {roc_auc:.4f}")

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Logistic Regression')
plt.legend(loc='lower right')
plt.show()

"""XGBOOST MODEL"""

import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the XGBoost Classifier
xgb_classifier = xgb.XGBClassifier(random_state=42)
xgb_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_xgb = xgb_classifier.predict(X_test)

# Evaluate the XGBoost model
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"XGBoost Accuracy: {accuracy_xgb}")
print(classification_report(y_test, y_pred_xgb))
print(confusion_matrix(y_test, y_pred_xgb))

"""TUNED XGBOOST MODEL"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Expanded parameter grid
param_grid_xgb = {
    'n_estimators': [150, 300],
    'learning_rate': [ 0.1, 0.2],
    'max_depth': [8, 9, 10],
    'subsample': [0.7, 0.8, 1.0]
}

# Use RandomizedSearchCV for efficiency
random_search_xgb = RandomizedSearchCV(
    estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    param_distributions=param_grid_xgb,
    n_iter=50,  # Adjust for more trials
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model
random_search_xgb.fit(X_train, y_train)

# Get best parameters
best_params_xgb = random_search_xgb.best_params_
best_score_xgb = random_search_xgb.best_score_

print(f"Best hyperparameters for XGBoost: {best_params_xgb}")
print(f"Best cross-validation score for XGBoost: {best_score_xgb}")

# Train final model with best parameters
best_xgb_classifier = xgb.XGBClassifier(**best_params_xgb, use_label_encoder=False, eval_metric='mlogloss', random_state=42)
best_xgb_classifier.fit(X_train, y_train)

# Evaluate on test data
y_pred_best_xgb = best_xgb_classifier.predict(X_test)
accuracy_best_xgb = accuracy_score(y_test, y_pred_best_xgb)

print(f"XGBoost Accuracy (with best hyperparameters): {accuracy_best_xgb}")
print(classification_report(y_test, y_pred_best_xgb))
print(confusion_matrix(y_test, y_pred_best_xgb))

"""GRADIENT BOOSTING MODEL"""

from sklearn.ensemble import GradientBoostingClassifier

# Initialize and train the Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier(random_state=42)
gb_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_gb = gb_classifier.predict(X_test)

# Evaluate the Gradient Boosting model
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print(f"Gradient Boosting Accuracy: {accuracy_gb}")
print(classification_report(y_test, y_pred_gb))
print(confusion_matrix(y_test, y_pred_gb))

"""TUNED GRADIENT BOOSTING MODEL"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define hyperparameter grid
param_grid_gb = {
     'n_estimators': [50, 100, 500],  # Number of boosting rounds
    'learning_rate': [ 0.1,  0.3],  # Step size
    'max_depth': [3, 9],  # Tree depth
    'min_samples_split': [2, 15],  # Min samples to split a node
    'min_samples_leaf': [1, 4],  # Min samples per leaf
    'subsample': [0.7, 0.9],  # Fraction of samples per iteration
    'max_features': ['sqrt', 'log2', None]  # Features per split
}

# Initialize RandomizedSearchCV
random_search_gb = RandomizedSearchCV(
    estimator=GradientBoostingClassifier(random_state=42),
    param_distributions=param_grid_gb,
    n_iter=50,  # Adjust for more trials
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model
random_search_gb.fit(X_train, y_train)

# Get best parameters and score
best_params_gb = random_search_gb.best_params_
best_score_gb = random_search_gb.best_score_

print(f"Best hyperparameters for Gradient Boosting: {best_params_gb}")
print(f"Best cross-validation score: {best_score_gb}")

# Train final model with best parameters
best_gb_classifier = GradientBoostingClassifier(**best_params_gb, random_state=42)
best_gb_classifier.fit(X_train, y_train)

# Evaluate on test data
y_pred_best_gb = best_gb_classifier.predict(X_test)
accuracy_best_gb = accuracy_score(y_test, y_pred_best_gb)

print(f"Gradient Boosting Accuracy (with best hyperparameters): {accuracy_best_gb}")
print(classification_report(y_test, y_pred_best_gb))
print(confusion_matrix(y_test, y_pred_best_gb))

import pandas as pd

data = {
    'Model': ['Logistic Regression', 'XGBoost', 'XGBoost (with best hyperparameters)', 'Gradient Boosting', 'Gradient Boosting (with best hyperparameters)'],
    'Accuracy': [accuracy, accuracy_xgb, accuracy_best_xgb, accuracy_gb, accuracy_best_gb]
}

df = pd.DataFrame(data)
df

"""# **MODEL BUILDING WITH OUTLIERS**"""

# Recreating DataFrame with outliers right after smote analysis for resampling
df_outliers = pd.DataFrame(X_resampled, columns=X.columns)
df_outliers['churn'] = y_resampled

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

X_outliers = df_outliers.drop('churn', axis=1)
y_outliers = df_outliers['churn']

# Scale numerical features
numerical_features = X_outliers.select_dtypes(include=np.number).columns
scaler4 = MinMaxScaler()
X_outliers_scaled = scaler4.fit_transform(X_outliers[numerical_features])
X_outliers_scaled = pd.DataFrame(X_outliers_scaled, columns=numerical_features, index=X_outliers.index)

df_outliers

"""FEATURE SELECTION

"""

def select_features_by_collinearity(df, threshold=0.85):
    corr_matrix = df.corr()
    correlated_features = set()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]
                correlated_features.add(colname)

    # Calculate feature importances (example using Random Forest)
    X = df.drop(columns=['churn'])  # Assuming 'churn' is your target variable
    y = df['churn']
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X, y)
    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)

    # Filter out correlated features and sort by importance
    filtered_importances = feature_importances.drop(correlated_features)

    return filtered_importances

# Call the function with your DataFrame and threshold
top_features = select_features_by_collinearity(df_outliers, threshold=0.85)

# Print or use the top features
print(top_features.sort_values(ascending=False))

X_ol = X_outliers_scaled[['day.mins', 'customer.calls', 'eve.mins', 'voice.plan', 'night.mins', 'account.length', 'intl.mins', 'night.calls', 'day.calls', 'eve.calls']]
y_ol = y_outliers

X_train_ol, X_test_ol, y_train_ol, y_test_ol = train_test_split(X_ol, y_ol, test_size=0.2, random_state=42)

"""TUNED GRADIENT BOOSTING MODEL"""

from sklearn.ensemble import GradientBoostingClassifier

# Initialize and train the Gradient Boosting Classifier
gb_classifier_ol = GradientBoostingClassifier(random_state=42)
gb_classifier_ol.fit(X_train_ol, y_train_ol)

# Make predictions on the test set
y_pred_gb_ol = gb_classifier_ol.predict(X_test_ol)

# Evaluate the Gradient Boosting model
accuracy_gb_ol = accuracy_score(y_test_ol, y_pred_gb_ol)
print(f"Gradient Boosting Accuracy with Outliers: {accuracy_gb_ol}")
print(classification_report(y_test_ol, y_pred_gb_ol))
print(confusion_matrix(y_test_ol, y_pred_gb_ol))

# Define hyperparameter grid
param_grid_gb_ol = {
     'n_estimators': [50, 100, 500],  # Number of boosting rounds
    'learning_rate': [ 0.1,  0.3],  # Step size
    'max_depth': [3, 9],  # Tree depth
    'min_samples_split': [2, 15],  # Min samples to split a node
    'min_samples_leaf': [1, 4],  # Min samples per leaf
    'subsample': [0.7, 0.9],  # Fraction of samples per iteration
    'max_features': ['sqrt', 'log2', None]  # Features per split
}

# Initialize RandomizedSearchCV
random_search_gb_ol = RandomizedSearchCV(
    estimator=GradientBoostingClassifier(random_state=42),
    param_distributions=param_grid_gb_ol,
    n_iter=50,  # Adjust for more trials
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model
random_search_gb_ol.fit(X_train_ol, y_train_ol)

# Get best parameters and score
best_params_gb_ol = random_search_gb_ol.best_params_
best_score_gb_ol = random_search_gb_ol.best_score_

print(f"Best hyperparameters for Gradient Boosting (with outliers): {best_params_gb_ol}")
print(f"Best cross-validation score: {best_score_gb_ol}")

# Train final model with best parameters
best_gb_classifier_ol = GradientBoostingClassifier(**best_params_gb_ol, random_state=42)
best_gb_classifier_ol.fit(X_train_ol, y_train_ol)

# Evaluate on test data
y_pred_best_gb_ol = best_gb_classifier_ol.predict(X_test_ol)
accuracy_best_gb_ol = accuracy_score(y_test_ol, y_pred_best_gb_ol)

print(f"Gradient Boosting Accuracy (with outliers and best hyperparameters): {accuracy_best_gb_ol}")
print(classification_report(y_test_ol, y_pred_best_gb_ol))
print(confusion_matrix(y_test_ol, y_pred_best_gb_ol))

data = {
    'Model': ['Logistic Regression', 'XGBoost', 'XGBoost (tuned)', 'Gradient Boosting', 'Gradient Boosting (tuned)'],
    'Accuracy_no_outliers': [accuracy,accuracy_xgb, accuracy_best_xgb, accuracy_gb, accuracy_best_gb],
    'Accuracy_with_outliers': ['NIL','NIL','NIL','NIL',accuracy_best_gb_ol]
}

df_results = pd.DataFrame(data)
df_results

import joblib

# Assuming your trained models are named logistic_model, xgboost_model, and gb_model
joblib.dump(logreg, "logistic_model.pkl")
joblib.dump(best_xgb_classifier, "tuned_xgboost_model.pkl")
joblib.dump(best_gb_classifier, "tuned_gradient_boosting_model.pkl")
joblib.dump(scaler, 'scaler.pkl')

"""#DEPLOYMENT"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# # DEPLOYMENT
# 
# import streamlit as st
# import joblib
# import numpy as np
# import pandas as pd
# 
# # Load trained models
# logistic_model = joblib.load("logistic_model.pkl")
# xgboost_model = joblib.load("tuned_xgboost_model.pkl")
# gb_model = joblib.load("tuned_gradient_boosting_model.pkl")
# scaler = joblib.load("scaler.pkl")  # Load the scaler
# 
# # Streamlit UI
# st.title("Telecom Churn Prediction App")
# st.write("Enter feature values to predict churn.")
# 
# # Model selection
# model_choice = st.selectbox("Choose a model:",
#                             ["Logistic Regression", "Tuned XGBoost", "Tuned Gradient Boosting (Highest Accuracy)"])
# 
# # User input fields (Top 10 Features Only)
# day_mins = st.slider("Day Minutes", min_value=0.0, max_value=300.0, step=0.1, value=150.0)
# customer_calls = st.slider("Customer Calls", min_value=0, max_value=20, step=1, value=5)
# eve_mins = st.slider("Evening Minutes", min_value=0.0, max_value=300.0, step=0.1, value=150.0)
# voice_plan = st.radio("Voice Plan", options=[0, 1], index=0)
# night_mins = st.slider("Night Minutes", min_value=0.0, max_value=300.0, step=0.1, value=150.0)
# account_length = st.slider("Account Length", min_value=1, max_value=243, step=1, value=100)
# intl_mins = st.slider("International Minutes", min_value=0.0, max_value=20.0, step=0.1, value=10.0)
# night_calls = st.slider("Night Calls", min_value=0, max_value=160, step=1, value=75)
# day_calls = st.slider("Day Calls", min_value=0, max_value=160, step=1, value=75)
# eve_calls = st.slider("Evening Calls", min_value=0, max_value=160, step=1, value=75)
# 
# # Store inputs in dictionary
# input_data = {
#     'day.mins': day_mins,
#     'customer.calls': customer_calls,
#     'eve.mins': eve_mins,
#     'voice.plan': voice_plan,
#     'night.mins': night_mins,
#     'account.length': account_length,
#     'intl.mins': intl_mins,
#     'night.calls': night_calls,
#     'day.calls': day_calls,
#     'eve.calls': eve_calls
# }
# 
# # Convert user input to DataFrame
# input_data = pd.DataFrame([input_data])
# 
# # Ensure input_data matches the training feature order
# expected_features = list(scaler.feature_names_in_)  # Get expected feature names from the scaler
# input_data = input_data.reindex(columns=expected_features, fill_value=0)  # Reorder and fill missing features
# 
# # Standardize input data
# input_data_scaled = scaler.transform(input_data)
# 
# # Ensure correct input shape
# input_data_scaled = np.array(input_data_scaled).reshape(1, -1)
# 
# # Prediction
# if st.button("Predict Churn"):
#     if model_choice == "Logistic Regression":
#         model = logistic_model
#     elif model_choice == "Tuned XGBoost":
#         model = xgboost_model
#     elif model_choice == "Tuned Gradient Boosting (Highest Accuracy)":
#         model = gb_model
# 
#     prediction = model.predict(input_data_scaled)
#     st.success(f"The predicted churn is {prediction[0]}")

!wget -q -O - ipv4.icanhazip.com

!npm install -g localtunnel

!streamlit run app.py & npx localtunnel --port 8501

